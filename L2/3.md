Okay, let's craft some concise and practical code snippets for Kafka Consumer and Producer using Spring for Apache Kafka.

**1. Producer (Publisher)**

*   **Core Concepts:**
    *   `KafkaTemplate`: Spring's primary class for sending messages to Kafka.
    *   Dependency: You will need `spring-kafka` dependency in your pom.xml or build.gradle

    ```xml
    <!-- Maven -->
        <dependency>
          <groupId>org.springframework.kafka</groupId>
          <artifactId>spring-kafka</artifactId>
        </dependency>
    ```
    ```groovy
    // Gradle
    implementation 'org.springframework.kafka:spring-kafka'
    ```

*   **Code Snippet:**

    ```java
    import org.springframework.kafka.core.KafkaTemplate;
    import org.springframework.beans.factory.annotation.Autowired;
    import org.springframework.stereotype.Service;

    @Service
    public class KafkaMessageProducer {

        @Autowired
        private KafkaTemplate<String, String> kafkaTemplate;

        private final String topicName = "my_topic"; // Configure in app properties

        public void sendMessage(String key, String message) {
            kafkaTemplate.send(topicName, key, message)
                .addCallback(
                    result -> System.out.println("Message sent successfully: " + message + ", partition: " + result.getRecordMetadata().partition()),
                    ex -> System.err.println("Error sending message: " + ex.getMessage())
                );
        }
    }
    ```

*   **Explanation:**
    *   `@Service`: Marks the class as a service for dependency injection.
    *   `@Autowired KafkaTemplate`: Injects the preconfigured `KafkaTemplate`.
    *   `sendMessage()`: Sends the key-value pair to the Kafka topic specified in `topicName`.
    *   `addCallback()`: Provides asynchronous success and failure handling.  Important for non-blocking operations.

*   **Configuration (application.properties/application.yml):**

   ```properties
    spring.kafka.bootstrap-servers=localhost:9092
    spring.kafka.producer.key-serializer=org.apache.kafka.common.serialization.StringSerializer
    spring.kafka.producer.value-serializer=org.apache.kafka.common.serialization.StringSerializer
    ```

    ```yaml
    spring:
        kafka:
            bootstrap-servers: localhost:9092
            producer:
              key-serializer: org.apache.kafka.common.serialization.StringSerializer
              value-serializer: org.apache.kafka.common.serialization.StringSerializer
    ```
    *   `bootstrap-servers`: Address(es) of Kafka brokers.
    *   `key-serializer` and `value-serializer`:  Defines how to serialize keys and values before sending to Kafka.

**2. Consumer (Subscriber)**

*   **Core Concepts:**
    *   `@KafkaListener`: Spring's annotation for marking a method as a Kafka consumer.
    *  `ConsumerRecord`: Object that encapsulates a record in Kafka.
     *   Dependency: You will need `spring-kafka` dependency in your pom.xml or build.gradle

*   **Code Snippet:**

    ```java
    import org.springframework.kafka.annotation.KafkaListener;
    import org.springframework.stereotype.Service;
    import org.apache.kafka.clients.consumer.ConsumerRecord;

    @Service
    public class KafkaMessageConsumer {

       @KafkaListener(topics = "my_topic", groupId = "my_group") // Configure in app properties
        public void consume(ConsumerRecord<String, String> record) {
           System.out.println("Received Message: Key: " + record.key() + ", Value: " + record.value() + ", Partition:" + record.partition() );
        }

     }
    ```

*   **Explanation:**
    *   `@KafkaListener`:  Marks the `consume()` method as a listener for the "my\_topic" topic and "my\_group" consumer group.
    *   `ConsumerRecord`: A consumer record will be consumed by the consumer by polling a record from a given partition that it has been assigned based on the `groupId`.
    *   Automatic deserialization: Spring handles deserializing data to String based on `value-deserializer` setting.

*   **Configuration (application.properties/application.yml):**
   ```properties
   spring.kafka.bootstrap-servers=localhost:9092
   spring.kafka.consumer.group-id=my_group
   spring.kafka.consumer.key-deserializer=org.apache.kafka.common.serialization.StringDeserializer
   spring.kafka.consumer.value-deserializer=org.apache.kafka.common.serialization.StringDeserializer
    ```

    ```yaml
    spring:
      kafka:
        bootstrap-servers: localhost:9092
        consumer:
          group-id: my_group
          key-deserializer: org.apache.kafka.common.serialization.StringDeserializer
          value-deserializer: org.apache.kafka.common.serialization.StringDeserializer
    ```
    *   `group-id`: Consumers with same `group-id` form a consumer group and will not receive duplicate messages.
    *   `key-deserializer` and `value-deserializer`: Defines how to deserialize keys and values after getting from Kafka.

**Key Points and Best Practices:**

*   **Error Handling:** Implement error handling and retry mechanisms for both producers and consumers.
*   **Asynchronous:** Kafka operations are asynchronous by nature. Use callbacks or futures to handle results non-blockingly.
*   **Configurations:** Externalize configurations (topic name, group id, broker addresses) using properties/yml files.
*   **Serialization/Deserialization:** Use appropriate serializers/deserializers based on your data (JSON, Avro, etc.).
*   **Consumer Groups:** Understand the role of consumer groups for scalability and load balancing. Each group will receive a copy of the message.
*  **Message Headers:** Spring Kafka supports sending and receiving headers in messages, which can be used for additional information or metadata.
*  **Consumer properties:** There are other properties like `auto.offset.reset` that can be set in the application.properties or yml to control the behavior of the consumer on how it retrieves the offset, when a new consumer group is created.
* **Concurrency:** For a single consumer, multiple threads can be created using the `concurrency` parameter in the `@KafkaListener`, when you need a higher throughput for the consumer side processing, but you still want to retain the ordering within the partition.

This gives you a practical starting point with Spring Kafka. Remember to adjust these snippets to your specific application's needs.
